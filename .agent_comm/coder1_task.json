{
  "agent_id": "coder1",
  "task_id": "task_6",
  "files": [
    {
      "name": "setup.py",
      "purpose": "Package installation setup",
      "priority": "low"
    }
  ],
  "project_info": {
    "project_name": "enhanced_cs.CL_2508.10860v1_From_Black_Box_to_Transparency_Enhancing_Automate",
    "project_type": "nlp",
    "description": "Enhanced AI project based on cs.CL_2508.10860v1_From-Black-Box-to-Transparency-Enhancing-Automate with content analysis. Detected project type: nlp (confidence score: 8 matches).",
    "key_algorithms": [
      "Infocom",
      "Error",
      "Personalized",
      "Each",
      "Improving",
      "Student",
      "Enhances",
      "Mean",
      "Final",
      "Over-Sampling"
    ],
    "main_libraries": [
      "torch",
      "numpy",
      "pandas"
    ]
  },
  "paper_content": "PDF: cs.CL_2508.10860v1_From-Black-Box-to-Transparency-Enhancing-Automate.pdf\nChunk: 1/1\n==================================================\n\n--- Page 1 ---\nFrom Black Box to Transparency: Enhancing Automated Interpreting\nAssessment with Explainable AI in College Classrooms\nZhaokun Jiang1Ziyin Zhang1*\n1Shanghai Jiao Tong University\nAbstract\nRecent advancements in machine learning have\nspurred growing interests in automated inter-\npreting quality assessment. Nevertheless, ex-\nisting research suffers from insufficient exami-\nnation of language use quality, unsatisfactory\nmodeling effectiveness due to data scarcity and\nimbalance, and a lack of efforts to explain\nmodel predictions. To address these gaps, we\npropose a multi-dimensional modeling frame-\nwork that integrates feature engineering, data\naugmentation, and explainable machine learn-\ning. This approach prioritizes explainability\nover \u201cblack box\u201d predictions by utilizing only\nconstruct-relevant, transparent features and con-\nducting Shapley Value (SHAP) analysis. Our\nresults demonstrate strong predictive perfor-\nmance on a novel English-Chinese consecutive\ninterpreting dataset, identifying BLEURT and\nCometKiwi scores to be the strongest predictive\nfeatures for fidelity, pause-related features for\nfluency, and Chinese-specific phraseological di-\nversity metrics for language use. Overall, by\nplacing particular emphasis on explainability,\nwe present a scalable, reliable, and transpar-\nent alternative to traditional human evaluation,\nfacilitating the provision of detailed diagnos-\ntic feedback for learners and supporting self-\nregulated learning advantages not afforded by\nautomated scores in isolation.\n1 Introduction\nInterpreting, or oral translation, is a complex yet\npivotal linguistic competency that offers extensive\neducational benefits by fostering advanced linguis-\ntic, communicational, cognitive, and emotional\ncapabilities (P\u00f6chhacker, 2001; Gile, 2021). It\nenhances active listening (Lee, 2013), oral profi-\nciency (Han and Lu, 2025), vocabulary acquisi-\ntion (Chen, 2024), and cross-cultural communica-\ntion (Stachl-Peier, 2020), while also strengthening\nhigher-order cognitive functions (Dong and Xie,\n*daenerystargaryen@sjtu.edu.cn2014) and anxiety management capabilities (Zhao,\n2022).\nGiven its multifaceted benefits, interpreting has\nincreasingly been recognized as both a valuable\npedagogical tool and the \u201cfifth skill\u201d (Mellinger,\n2018) alongside listening, speaking, reading, and\nwriting. The intricate nature of interpreting ne-\ncessitates a continuous cycle of structured practice,\nrigorous assessment, and diagnostic feedback (Gile,\n2021). However, traditional human-based assess-\nment often requires raters to simultaneously con-\nsult the source text, the interpreted output, and\ndetailed rating scales, a cognitively demanding pro-\ncess that increases the risk of scoring bias and in-\nconsistency (Lee, 2019; Han et al., 2024).\nThe inherent limitations of human evaluation\nhave spurred considerable interest in automated\nassessment. However, existing works are charac-\nterized by both a thematic imbalance and method-\nological constraints. Among the three established\ndimensions of interpreting quality (fidelity, flu-\nency, and language use), investigations have dis-\nproportionately focused on the first two , while\nlanguage use has received scant scholarly atten-\ntion (Yu and van Heuven, 2017; Han and Yang,\n2023; Wang and Wang, 2022; Han and Lu, 2021;\nLu and Han, 2022). Furthermore, prior research\nhas predominantly relied on conventional statistical\nmethods such as correlation and regression analy-\nses (Yu and van Heuven, 2017; Wang and Wang,\n2022; Han and Lu, 2021; Lu and Han, 2022), which\nare based on assumptions of linearity that often do\nnot hold in complex, real-world datasets.\nThe advent of machine learning (ML) algorithms\nand large language models (LLMs) presents novel\nopportunities to analyze complex data patterns that\nelude traditional statistical methods. Nevertheless,\na notable obstacle in their application is the se-\nvere imbalance in data composition . Wang and\nYuan (2023), for example, find their five-class clas-\nsification model unable to identify performances at\n1arXiv:2508.10860v1  [cs.CL]  14 Aug 2025\n\n--- Page 2 ---\nFigure 1: SHAP-Based global feature importance for InfoCom (left), FluDel (middle), and TLQual (right) predic-\ntions. Warmer tones (e.g., red) signify higher feature values and cooler tones (e.g., blue) indicate lower feature\nvalues. The features are arranged in descending order along the y-axis based on their global importance. The\nmeaning of FluDel and TLQual features are given in Table 2, 3 respectively.\nthe distributional extremes (\u201cvery poor\u201d and \u201cvery\ngood\u201d), a direct consequence of the imbalanced\ntraining data distribution. Another limitation is\nthe inherent opacity of automated scoring sys-\ntems . Jia and Aryadoust (2023), for instance, find\nmoderate correlations between GPT-4\u2019s interpret-\ning performance assessment and human-assigned\nscores. Crucially, the internal decision-making pro-\ncesses of the LLMs remained opaque, with only\nthe final scores being accessible. This \u201cblack box\u201d\nnature severely restricts the diagnostic and educa-\ntional utility of LLM scores.\nIn response to these challenges, we raise the\nfollowing questions in this work:\n1) Can we mitigate the underperformance of in-\nterpreting assessment models with data augmenta-\ntion?\n2) Which specific features of fidelity, fluency,\nand language use exhibit the strongest predictive\npower in interpreting assessment models?\n3) What specific feature combinations influence\nindividual student scores for each dimension of\ninterpreting quality?\nTo answer these questions, we introduce a novel\napproach that combines feature engineering, data\naugmentation, and explainable AI (XAI) tech-\nniques (Arrieta et al., 2019; Linardatos et al.,\n2020) to evaluate interpreting performance across\nthree key dimensions: fidelity, fluency, and target\nlanguage quality. After using Variational Auto-\nEncoders (V AEs) to augment the data, we extract a\nbroad set of features including translation quality\nmetrics, temporal measures, and syntactic com-\nplexity indices to predict interpreting performance.\nBased on these features, we predict performance\nseparately for each of the three dimensions adopt-\ning a multi-dimensional modeling strategy, which\nfacilitates a more fine-grained analysis of interpret-\ning quality and provides clearer insights into thespecific contributions of features to each criterion.\nFurthermore, we apply Shapley Value (SHAP) anal-\nysis to provide interpretable explanations at both\nglobal and individual levels. To the best of our\nknowledge, we represent the first systematic ef-\nforts to automate the assessment of target language\nquality in interpreting.\n2 Related Work\n2.1 Automated Interpreting Assessment\nThe field of automated interpreting assessment is\nwitnessing a paradigm shift, moving from statisti-\ncal methods toward more sophisticated neural mod-\nels. To date, the application of ML to interpreting\nquality evaluation remains a nascent but growing\ndomain. The pioneering work by Le et al. (2016)\ndeveloped estimators based on features from auto-\nmatic speech recognition (ASR) and machine trans-\nlation (MT), finding that MT features are most influ-\nential in predicting interpretation quality. Follow-\ning that, Stewart et al. (2018) adapted the QuEst++\nquality estimation pipeline with Support Vector\nRegression to predict the performance of simulta-\nneous interpreters. More recently, Wang and Yuan\n(2023) employed SVM and KNN algorithms to\nclassify E-C interpretations, while Han et al. (2025)\nfurther advanced the domain by integrating neural-\nbased metrics with acoustic and linguistic indices\nthrough ordinal logistic regression.\n2.2 Dimensions of Interpreting Assessment\nInformation Completeness Information com-\npleteness, also known as fidelity, refers to the extent\nof informational, semantic, and pragmatic corre-\nspondence between a source message and its trans-\nlation (Han, 2018). Existing metrics for automatic\nfidelity assessment can be broadly categorized into\ntwo types: non-neural and neural-based.\nNon-neural metrics such as BLEU (Papineni\n2\n\n--- Page 3 ---\net al., 2002) and chrF (Popovic, 2015) mainly\nrely on statistical and lexical matching to quan-\ntify the overlap of word or character sequences\nbetween a candidate translation and a human ref-\nerence. Although these metrics have been widely\nadopted in the past decades, they have also been\ncriticized for their reliance on surface-level compar-\nisons that may not capture deeper semantic equiva-\nlence (Castilho et al., 2018).\nIn contrast, neural-based metrics are derived\nfrom pre-trained language models and tran-\nscend surface matching by comparing contextu-\nalized embeddings. Prominent examples include\nBERTScore (Zhang et al., 2020), BLEURT (Sel-\nlam et al., 2020), CometKiwi (Rei et al., 2022),\nand xCOMET (Guerreiro et al., 2024). While Han\nand Lu (2025) report a strong aggregate correlation\nbetween these scores and human evaluations on\nE-C interpreting, Lu and Han (2022) find that the\nnon-neural metrics BLEU and NIST outperform\nBERTScore, suggesting that non-neural and neu-\nral metrics may capture distinct, and potentially\ncomplementary, facets of interpreting quality.\nFluency Fluency is another key dimension of in-\nterpreting quality, reflecting how effectively and\nnaturally an interpretation is delivered (Stenzl,\n1983). In computational modeling, fluency fea-\ntures are typically classified into three cate-\ngories (Tavakoli and Skehan, 2005): (1) speed flu-\nency, which captures the rate and density of deliv-\nery; (2) breakdown fluency, which measures speech\ncontinuity through the absence of interruptions and\npauses; and (3) repair fluency, which quantifies\nself-corrections and repetitions.\nWithin interpreting empirical research, consider-\nable evidence has underscored the high predictive\npower of speed fluency features such as speech rate,\nphonation time ratio, and articulation rate (Han and\nYang, 2023; Han, 2015; Song, 2020; Yu and van\nHeuven, 2017), while other works have also identi-\nfied breakdown fluency features (e.g. mean length\nof unfilled pauses) as strong predictors (Wang and\nWang, 2022; Wu, 2021). In contrast, repair fluency\nfeatures are less commonly employed and seldom\nshow strong predictive effectiveness (Han, 2015).\nTarget Language Use In interpreting assess-\nment, target language quality typically refers to\nthe grammaticality and idiomaticity of the target\nlanguage output (Han, 2018). Automated assess-\nment of this dimension is facilitated by advances in\ncomputational tools such as Coh-Metrix (Graesseret al., 2004), TAASSC (Kyle, 2016), L2SCA (Lu,\n2010), and CCA (Hu et al., 2022b,a), which oper-\nationalize linguistic quality by calculating a wide\narray of features from lexical and phraseological\nindices to measures of syntax and discourse. While\nthese features have been extensively applied in L2\nwriting and speaking research (Lu, 2010; Kyle and\nCrossley, 2017; Chen et al., 2018), their applica-\ntion to translation and interpreting contexts remains\nnascent, though existing findings show consider-\nable promise (Ouyang et al., 2021; Han et al., 2025,\n2022).\nYet, two key challenges remain. The first is the\nneed for more fine-grained feature design and\napplication . While coarse-grained metrics like\nT-unit complexity have long been valued (Ortega,\n2003), recent research advocates for supplementing\nthem with fine-grained, usage-based indices that\ncan capture subtle structural variations and better\npredict language development (Norris and Ortega,\n2009; Kyle and Crossley, 2017). The second chal-\nlenge concerns language specificity , as most NLP\ntools are developed primarily for English and may\nnot fully account for the linguistic characteristics\nof other languages, such as the lack of overt mor-\nphological inflections and unique phraseological\nconstructions in Chinese (Li and Thompson, 1989;\nHu et al., 2022b)\nThis evolving landscape is further complicated\nby the advent of LLMs. A recent large-scale study\nby Zhang et al. (2024b) demonstrates that GPT-4o\nachieves near-human accuracy in grammatical ac-\nceptability judgment, leading to questions in the\noptimal combination of analytical tools \u2014 from es-\ntablished linguistic indices to emergent LLM-based\njudgments \u2014 that offers the most robust predictive\npower for assessing language use in interpreting.\n2.3 Data Augmentation in Interpreting\nAssessment\nDespite the aforementioned results in automatic\ninterpreting assessment, the empirical application\nof ML is hampered by two fundamental and in-\nterrelated data challenges in this domain: small\nsample size and imbalanced data composition .\nThe field is largely characterized by studies that\nrely on small datasets (Yu and van Heuven, 2017;\nLu and Han, 2022; Wang and Yuan, 2023; Wang\nand Wang, 2022), substantially increasing the risk\nof overfitting. This problem is further exacerbated\nby pronounced class imbalance, as most datasets\nare heavily skewed toward average performance,\n3\n\n--- Page 4 ---\nwith markedly fewer samples representing either\nvery high or very low quality (Wang and Yuan,\n2023; Han et al., 2025).\nTo surmount these obstacles, data augmentation\nhas emerged as a critical methodological interven-\ntion capable of enhancing model robustness and va-\nlidity (Mumuni and Mumuni, 2022). Common aug-\nmentation approaches include perturbation-based\nmethods (adding Gaussian noise), interpolation\ntechniques like SMOTE (Chawla et al., 2002),\nand generative models such as Generative Ad-\nversarial Networks (GANs) and Variational Au-\ntoencoders (V AEs) (Mumuni and Mumuni, 2022).\nAmong these, V AE offers three key advantages for\nML-based interpreting assessment (Kingma and\nWelling, 2014). First, its probabilistic framework\ncaptures complex interdependencies within fidelity,\nfluency, and language use features. Second, the\ncontinuous latent space enables smooth interpo-\nlation between existing samples to create coher-\nent variations. Third, V AE preserves feature-label\ncorrespondence (i.e., the direct link between each\nsample\u2019s features and its corresponding interpret-\ning quality score), which is crucial for maintaining\nassessment validity. Zhang et al. (2024a) also de-\nmostrate the empirical viability of this technique.\n2.4 Explainable AI (XAI) and Its Application\nin Educational Contexts\nAs educational AI systems become more sophisti-\ncated, XAI techniques are essential for understand-\ning and validating these systems, thereby ensuring\nreliability, trust, and fairness (Gilpin et al., 2018;\nRudin, 2018).\nCurrent XAI techniques fall into two main cate-\ngories: intrinsic and post-hoc approaches (Gilpin\net al., 2018; Rudin, 2018; Arrieta et al., 2019;\nLinardatos et al., 2020). Intrinsic methods priori-\ntize inherent interpretability by using transparent\nmodel architectures such as rule-based systems, de-\ncision trees, and linear models where coefficients\ndirectly indicate feature influence. In contrast,\npost-hoc methods explain already-trained black-\nbox models without altering their structure, pro-\nviding insights into complex models that would\notherwise remain opaque. Most popular post-hoc\nmethods - such as SHAP (Lundberg and Lee, 2017)\nand LIME (Ribeiro et al., 2016) - provide fea-\nture attribution, while other methods also exist for\nexample-based explanations and counterfactual ex-\nplanations (Arrieta et al., 2019; Linardatos et al.,\n2020). Based on their scope, post-hoc methods\nFigure 2: Methodological workflow of this study.\ncan also be categorized as either global explanation\n(illuminating overall model behavior across all in-\nstances) or local explanation (clarifying individual\npredictions).\nFor XAI research in education, learning analyt-\nics represents the most substantial area (Parkavi\net al., 2024; Balachandar and Venkatesh, 2024),\nwhile applications have also been seen in auto-\nmated language assessment, with most studies con-\ncentrating on explaining factors influencing perfor-\nmance quality (Kumar and Boulanger, 2020; Tang\net al., 2024). To our knowledge, Wang (2024) is\nthe only existing work to focus on explainability\nin automated interpreting assessment, which classi-\nfies interpreting quality into 5 levels and provides\nglobal explanations of feature importance using\ncorrelation analysis.\n3 Method\nAs illustrated in Figure 2, this study follows a struc-\ntured method. First, we compile a new dataset\ncomprising 117 student interpreting recordings in\nthe English-Chinese direction, from which a range\nof linguistically meaningful and theoretically moti-\nvated features are extracted. To address challenges\nrelated to the small sample size and imbalanced\nscore distribution, we employ V AE to generate\nnew, realistic samples (Kingma and Welling, 2014).\nAfter that, several machine learning models are\ntrained to predict interpreting quality scores across\ndifferent dimensions. Finally, to explain the inner\ndecision-making of the trained models, we conduct\na series of SHAP analyses.\n3.1 Original Dataset\nWe compile a new dataset of 117 English-Chinese\nconsecutive interpreting samples, collected from\n39 undergraduate English majors at a university in\nShanghai, China (Mean age = 18.47 years, SD =\n1.13 years). All participants, whose L1 is Chinese\n4\n\n--- Page 5 ---\nand L2 is English, have passed CET-4 (College\nEnglish Test), demonstrating satisfactory English\nproficiency. Before data collection, they completed\n16 weeks (32 credit hours) of interpreting training.\nThe interpreting task uses six passages adapted\nfrom authentic public speeches, each containing an\nequal number of sentences and controlled for sen-\ntence length (M = 18.14 words, SD = 0.78 word).\nFurther details about the passages, along with ex-\ntracted linguistic feature values, are presented in\nAppendix A. These texts are converted into audio\nformat using ElevenLabs\u2019 text-to-speech technol-\nogy1. The resulting audio files feature standard\npronunciation and averaged approximately 2 min-\nutes in duration.\nAssessment of the interpreting samples is con-\nducted by three experienced raters, each with over\nthree years of university-level teaching experience\nin domestic or international settings. The evalua-\ntion employes Han (2018)\u2019s four-band, eight-point\nanalytic rubric, which assesses the three key dimen-\nsions of interpreting quality: InfoCom (informa-\ntion completeness), FluDel (fluency), and TLQual\n(target language quality). To ensure scoring consis-\ntency, raters underwent comprehensive training be-\nfore the formal assessment. Detailed descriptions\nof the rater training procedures, student separa-\ntion reliability, and the infit and outfit mean square\nstatistics for each rater are provided in Appendix B.\nTo mitigate potential inconsistencies and rater bias,\nMany-Facet Rasch Measurement (MFRM) analy-\nsis (Linacre, 2002) is used to calibrate raw scores\nand establish the final ground truth scores.\n3.2 Audio Processing\nTo process the audio recordings of interpreting, we\nfirst use iFLYTEK ASR system2to transcribe them\ninto texts. To enhance annotation reliability, we\nimplement a two-stage error detection process.\nIn the first stage, GPT-4o3is used for grammati-\ncal error diagnosis by adapting the framework of\nRao et al. (2020) and Fu et al. (2018). A structured\nprompt template (see Appendix C) is designed to\nguide GPT-4o\u2019s annotations, providing explicit in-\nstructions on four major error types: Redundant\nWords (R), Missing Words (M), Word Selection\nErrors (S), and Word Ordering Errors (W). To en-\nhance the model\u2019s performance and reliability, we\n1https://elevenlabs.io/\n2https://global.xfyun.cn/products/\nreal-time-asr\n3GPT-4o-2024-08-06 with temperature set to 0.provide the it with few-shot examples and instructe\nit to explicitly articulate its decision-making pro-\ncess for each identified error and provide a corre-\nsponding confidence level. Particularly, we specify\nin the guidelines that filled pauses (e.g., \u201cuh\u201d) are\nnot considered errors, and analysis should focus\nsolely on the final sentence version, disregarding\nrepetitions, false starts, or self-corrections.\nIn the second stage, each transcription is manu-\nally reviewed and corrected. We recruited two post-\ngraduate students in linguistics to independently\nannotate 100 randomly selected sentences, follow-\ning the same guidelines as GPT. Inter-annotator\nagreement among human annotators yields a Co-\nhen\u2019s Kappa coefficient of 0.86, while agreement\nbetween GPT-4o annotations and human annota-\ntions achieves a Fleiss\u2019 Kappa coefficient of 0.71,\nindicating a substantial level of consistency.\n3.3 Feature Extraction\nEach scoring dimension of interpreting is rep-\nresented by a distinct set of extracted features.\nFluency features are extracted from the original\ntranscript, while other features are derived from\ncleaned transcripts (after removing fillers, false\nstarts, and self-repair).\nForInfoCom , we use five established metrics\nfrom the field of machine translation quality assess-\nment to measure the preservation of information\nfrom source to target language (Table 1).\nFluDel features include 14 temporal features (Ta-\nble 2) derived from prior research (Barik, 1973;\nYu and van Heuven, 2017; Song, 2020; Wang and\nWang, 2022). These features can be categorized\ninto two groups: speed fluency features (1\u20136) and\nbreakdown fluency features (7\u201314). Features re-\nlated to unfilled pauses are extracted automatically\nusing Python packages librosa (v0.10.2) and sound-\nfile (v0.12.1), with pauses identified based on an\nintensity threshold of -18 dB as recommended by\nWu (2021). Additional features are derived from\ntime-aligned transcriptions generated by the iFLY-\nTEK ASR system.\nTLQual is evaluated through 25 features related\nto syntactic complexity and grammatical accuracy.\nAmong them, 21 syntactic complexity features en-\ncompassing both coarse-grained and fine-grained\nmeasures (Table 3) are extracted using Chinese\nCollocation Analyzer (CCA, Hu et al., 2022b,a),\nwhich is specifically developed for L2 Chinese\ntexts, making it particularly appropriate for E-C\ninterpreting studies. The remaining 4 grammatical\n5\n\n--- Page 6 ---\nFeature Short description\nchrF Measures n-gram overlap between the interpreted and reference text\nBLEURT-20Assesses the semantic similarity between the interpreted text and reference text based\non contextualized embeddings from BERT and RemBERT\nBERTScoreMeasures the similarity between interpreted and reference translations by computing\ncosine similarity of their contextualized embeddings using BERT\nCometKiwi-daA reference-free regression model based on the InfoXLM architecture, trained on\ndirect assessments from WMT17-WMT20 and the MLQE-PE corpus\nxCOMET-XLAn extension of COMET, designed to identify error spans and assign quality scores,\nachieving state-of-the-art correlation with MQM error typology-derived scores\nTable 1: Features adopted for InfoCom assessment.\nFeature Full Name Description\nSR Speech RateThe overall pace of speech, calculated as the number of syllables uttered\nper second.\nAR Articulation Rate The rate of syllable production, excluding pauses.\nPTR Phonation Time Ratio The proportion of time spent vocalizing relative to the total duration.\nMLSMean Length of Sylla-\nblesThe average duration of each syllable.\nMLR Mean Length of Run The average number of syllables produced in a continuous stream.\nPSC Pruned Syllable Count The total syllable count after removing filled pauses.\nNFPNumber of Filled\nPausesThe frequency of filled pauses (e.g., \u201cum,\u201d \u201cuh\u201d).\nNUPNormalized Number of\nUnfilled PausesThe frequency of silent pauses. An unfilled pause is defined as a silence\nof 0.35 seconds or longer, consistent with recommendations for E-C\ninterpreting (Mead, 2005).\nMLFPMean Length of Filled\nPausesThe average duration of filled pauses.\nMLUPMean Length of Un-\nfilled PausesThe average duration of silent pauses.\nNRLFPNumber of Relatively\nLong Filled PausesThe number of filled pauses longer than Q3 + 1.5 * Interquartile Range\n(IQR) and shorter than or equal to Q3 + 3 * IQR.\nNRLUPNumber of Relatively\nLong Unfilled PausesThe number of unfilled pauses longer than Q3 + 1.5 * IQR and shorter\nthan or equal to Q3 + 3 * IQR.\nNRSANumber of Relatively\nSlow ArticulationsThe number of syllables longer than Q3 + 1.5 * IQR and shorter than or\nequal to Q3 + 3 * IQR.\nNPSANumber of Particularly\nSlow ArticulationsThe number of syllables longer than Q3 + 3 * IQR.\nTable 2: 14 FluDel features examined in this work.\naccuracy features are derived from the grammatical\nerror annotations by GPT-4o, specifically Number\nof Redundant Words (NRW), Number of Missing\nWords (NMW), Number of Word Selection Errors\n(NWSE), and Number of Word Ordering Errors\n(NWOE).\n3.4 Data Augmentation\nUnlike general L2 learners, interpreting students\nconstitute a smaller pool due to the advanced\nlinguistic competence and cognitive demands re-\nquired by the task. This scarcity underscores the\nneed for data augmentation techniques to increase\nthe quantity and diversity of learner datasets (Mu-muni and Mumuni, 2022).\nIn line with the approach proposed by Zhang\net al. (2024a), we employ Variational Autoencoder\n(V AE) to address the challenge of score distribu-\ntion imbalance in the original dataset. The primary\nobjective is to generate realistic, synthetic feature\nvectors for the three distinct dimensions of inter-\npreting quality being assessed. To achieve this, we\ntrain a separate conditional V AE for each of the\nthree dimensions. The synthetic feature vectors\ngenerated by these V AE models are then combined\nwith the original 117 data points, resulting in an\naugmented dataset comprising 500 samples.\n6\n\n--- Page 7 ---\nCoarse-Grained Phraseological Diversity Phraseological complexity\nMean Length of Sentences\n(MLS)Verb-Object Root Type-Token Ratio\n(VO_RTTR)Verb-Object Combination Ratio\n(VO_RATIO)\nMean Length of T-units (MLTU)Subject-Predicate Root Type-Token\nRatio (SP_RTTR)Subject-Predicate Combination Ra-\ntio (SP_RATIO)\nNumber of T-units Per Sentence\n(NTPS)Adjective-Noun Root Type-Token\nRatio (AN_RTTR)Adjective-Noun Combination Ratio\n(AN_RATIO)\nMean Length of Clauses (MLC)Adverb-Preposition Root Type-\nToken Ratio (AP_RTTR)Adverb-Preposition Combination\nRatio (AP_RATIO)\nNumber of Clauses Per Sentence\n(NCPS)Classifier-Noun Root Type-Token\nRatio (CN_RTTR)Classifier-Noun Combination Ratio\n(CN_RATIO)\nPreposition-Postposition Root Type-\nToken Ratio (PP_RTTR)Preposition-Postposition Combina-\ntion Ratio (PP_RATIO)\nPreposition-Verb Root Type-Token\nRatio (PV_RTTR)Preposition-Verb Combination Ratio\n(PV_RATIO)\nPredicate-Complement Root Type-\nToken Ratio (PC_RTTR)Predicate-Complement Combina-\ntion Ratio (PC_RATIO)\nTable 3: 21 Syntactic complexity features adopted for TLQual assessment.\n3.5 Model Training and Validation\nThree types of machine learning models \u2014 XG-\nBoost, Random Forest (RF), and Multi-Layer Per-\nceptron (MLP) \u2014 are employed to predict the In-\nfoCom, FluDel, and TLQual scores. The modeling\nprocess followes a systematic procedure that con-\nsists of feature extraction, feature standardization,\ndata splitting, model training and validation, and\nmodel testing (Mienye and Sun, 2022).\nAll extracted features (as detailed in Section 3.3)\nare first standardized using z-score normalization.\nThe initial dataset is then split into training (80%)\nand testing (20%) subsets. Following the data\nsplit, model training and validation are conducted\nwith five-fold cross-validation and a grid search\nfor hyperparameters, using root mean square error\n(RMSE) as validation criterion.\nAfter cross-validation and hyperparameter opti-\nmization, the best-performing configuration is se-\nlected for each model. Each final model is then re-\ntrained on the entire training set using the optimal\nhyperparameters and subsequently evaluated on\nthe held-out test set to assess its predictive perfor-\nmance on unseen data. Multiple evaluation metrics\nare employed in this stage to provide a comprehen-\nsive assessment of model quality, including:\n(1) RMSE: measures the magnitude of prediction\nerrors.\n(2) Spearman\u2019s ( \u03c1): assesses the monotonic rela-\ntionship between predicted and actual scores.\n(3) Mean absolute error (MAE): quantifies the\naverage absolute deviation between predicted and\nactual scores, providing a direct measure of predic-tion accuracy.\n(4) Mann-Whitney U Test: determines whether\nthere are significant differences in the distributions\nof predicted and actual scores.\n(5) Exact Agreement rate (EAR): quantifies the\nproportion of predictions that exactly match the\nactual scores after both are rounded to the nearest\ninteger. Rounding is required because our models\npredict continuous MFRM-calibrated scores (1-8),\nand agreement is typically assessed against discrete\nlevels.\n(6) Adjacent Agreement rate (AAR): measures\nthe proportion of predictions that fall within one\ninteger unit (either +1 or -1) of the actual scores\nafter both are rounded to the nearest integer.\nBeyond these overall metric values, we also\nperform case studies of prediction errors to gain\nmore in-depth insights into specific aspects of the\nmodel\u2019s performance.\n3.6 Result Explanation Using XAI Techniques\nWe further employ SHAP to interpret model be-\nhavior at two levels: the overall model (global\nexplanations) and individual predictions (local ex-\nplanations). Global explanations offer a broader\nperspective by summarizing the overall impact of\nfeatures across the entire dataset. Local explana-\ntions, on the other hand, provide insights into how\nindividual features influence a single predicted out-\ncome. These analyses are implemented using the\nshap library4.\n4https://shap.readthedocs.io/en/latest/index.\nhtml\n7\n\n--- Page 8 ---\nFigure 3: Pairwise correlation heatmap between features and scores.\n4 Results\n4.1 Descriptive Statistics\nDue to the consistently reasonable level of inter-\npreting proficiency demonstrated by all student par-\nticipants, the dataset lacks samples with scores in\nthe 1-2 range. However, Figure 4 demonstrates\nthat data augmentation has successfully achieved\nan approximately uniform distribution of interpreta-\ntion scores on the remaining range. Table 4 further\nreveals that compared with the original data, the\naugmented data exhibits very close mean values\nand marginally insreased standard deviations in\nall three dimensions. The descriptive statistics for\nall features used in this work are provided in Ap-\npendix D, and the pairwise Spearman\u2019s correlations\nbetween features and scores in both the original and\naugmented datasets are illustrated in Figure 3.\nScore Mean SD Skewness Kurtosis\nInfoComRaw 5.32 1.35 -0.37 2.25\nAug. 5.33 1.47 -0.05 -0.51\nFluDelRaw 4.93 0.77 -0.31 2.94\nAug. 4.95 0.98 -0.10 -0.67\nTLQualRaw 5.21 0.95 -0.23 3.38\nAug. 5.24 1.06 0.06 -0.85\nTable 4: Descriptive statistics for scores from the raw\ndata and augmented data.\n4.2 Effectiveness of Models Trained on Raw\nand Augmented Data\nAs shown in Table 5, XGBoost trained on the aug-\nmented dataset achieves the highest performance\nFigure 4: Distribution of raw (left), generated (middle),\nand augmented (right) data.\nin predicting FluDel and TLQual scores, represent-\ning an improvement over its already robust perfor-\nmance on the raw dataset. For InfoCom prediction,\nthe RF regressor trained on augmented data yields\nthe best results, also substantially outperforming\nthe same model trained on raw data. In contrast,\nMLP consistently exhibits the lowest performance,\nthough also showing a notable improvement when\ntrained on augmented data. In Appendix E, we\nprovide detailed analyses of instances where model\npredictions diverge greatly from human scores, of-\nfering nuanced insights into the models\u2019 perfor-\n8\n\n--- Page 9 ---\nScore Model Data RMSE Spearman MAE Mann-Whitney U EAR AAR\nInfoComXGBoostraw 1.36 0.49\u2217\u22170.95 259 (p = 0.70) 0.63 0.83\naug. 1.17 0.62\u2217\u22170.49 5751 (p = 0.12) 0.71 0.86\nRFraw 1.42 0.51\u2217\u22170.87 209 (p = 0.45) 0.67 0.88\naug. 1.05 0.68\u2217\u22170.41 5693 (p = 0.15) 0.77 0.90\nMLPraw 2.43 0.43\u22171.21 215 (p = 0.53) 0.54 0.75\naug. 1.25 0.58\u2217\u22170.79 5744 (p = 0.12) 0.68 0.77\nFluDelXGBoostraw 0.84 0.69\u2217\u22170.65 272 (p = 0.49) 0.69 0.83\naug. 0.68 0.87\u2217\u22170.41 5375 (p = 0.36) 0.72 0.91\nRFraw 0.70 0.65\u2217\u22170.68 274 (p = 0.46) 0.71 0.83\naug. 0.61 0.86\u2217\u22170.43 5302 (p = 0.46) 0.75 0.93\nMLPraw 1.74 0.39\u2217\u22171.17 274 (p = 0.46) 0.54 0.71\naug. 1.20 0.53\u2217\u22170.89 4621 (p = 0.36) 0.64 0.82\nTLQualXGBoostraw 0.87 0.66\u2217\u22170.72 267 (p = 0.41) 0.67 0.83\naug. 0.75 0.79\u2217\u22170.45 5386 (p = 0.33) 0.76 0.91\nRFraw 0.97 0.58\u2217\u22170.86 232 (p = 0.42) 0.63 0.79\naug. 0.92 0.73\u2217\u22170.54 5522 (p = 0.20) 0.78 0.89\nMLPraw 1.58 0.45\u22171.10 206 (p = 0.40) 0.58 0.75\naug. 1.04 0.62\u2217\u22170.83 4973 (p = 0.95) 0.69 0.85\nTable 5: Performance of machine learning regressors trained on raw and augmented data.\u2217\u2217p <0.01;\u2217p <0.05.\nmance characteristics.\n4.3 Global explanations of model prediction\nFigure 1 (left) illustrates the global feature impor-\ntance of the best-performing RF regressor for In-\nfoCom score prediction. Among these, BLEURT\n(M = 0.32, 95% CI5= [0.25, 0.37]), CometKiwi\n(M = 0.17, 95% CI = [0.08, 0.26]), and chrF (M\n= 0.07, 95% CI = [0.04, 0.09]) demonstrate the\nhighest mean SHAP values. In other words, higher\nvalues of these metrics are positively associated\nwith higher predicted InfoCom scores.\nAs illustrated in Figure 1 (middle), NFP (M = -\n0.17, 95% CI = [-0.27, -0.10]) exhibits the strongest\nnegative effect on FluDel scores, with higher NFP\nvalues leading to lower predictions by the XGBoost\nregressor. Similarly, other breakdown fluency fea-\ntures, including MLUP, NUP, and MLFP also neg-\natively impact predicted outcomes. Speed fluency\nfeatures such as PSC, SR, PTR, and MLS have\na positive but very small impact on the model\u2019s\n5To assess the stability of feature contributions, a bootstrap\nprocedure is conducted with 1,000 resamples drawn from the\naugmented dataset. For each bootstrap sample, SHAP values\nare computed using the best-performing ML model. The mean\nSHAP value for each feature is recorded across iterations to\nestimate its average effect on predictions. 95% Confidence\nintervals (CI) are calculated as the 2.5th and 97.5th percentiles\nof the bootstrapped distribution, capturing both the direction\nand magnitude of each feature\u2019s influence.predictions, while MLR yields a negative effect\ninstead.\nFigure 1 (right) demonstrates that the grammati-\ncal accuracy index NWSE (M = -0.09, 95% CI = [-\n0.15, -0.04]) has an inverse relationship with model\npredictions, indicating that a higher frequency of\nword selection errors corresponds to lower pre-\ndicted scores. Among phraseological complex-\nity features, CN_RATIO (M = 0.25, 95% CI =\n[0.18, 0.31]) has the most significant influence,\nwith higher values leading to increased predictions.\nIn addition, a group of phraseological diversity\nmetrics also contribute positively to model output,\nincluding PP_RTTR and PV_RTTR. In contrast,\nAP_RTTR and PC_RTTR exhibit negative effects.\nFor coarse-grained features, higher MLC values\nare associated with lower model predictions, while\nMLS positively influences predicted outcomes.\n4.4 Local explanations of model prediction\nFigure 5 illustrates the SHAP force plot for the\nInfoCom prediction of Sample 25, providing a de-\ntailed depiction of individual feature contributions.\nThe plot is centered around the base value (approx-\nimately 5.4), representing the mean model output\nacross the training dataset. The cumulative con-\ntributions of the InfoCom features slightly elevate\nthe prediction to 5.66. Among these, BLEURT\n9\n\n--- Page 10 ---\nFigure 5: SHAP force plot for the InfoCom prediction of Sample 25.\nFigure 6: SHAP waterfall plot for the predicted FluDel\nscore of Sample 50.\nand COMET-Kiwi exert the most significant posi-\ntive influence, whereas chrF contributes negatively.\nThe relatively high BLEURT and COMET-Kiwi\nscores suggest that Sample 25 retains most of the\nsource information, albeit with some loss, while\nthe markedly low chrF score indicates substantial\nlexical and syntactic divergence from the reference\ntext.\nIn Figure 6, the SHAP waterfall plot for the\nFluDel prediction of Sample 50 is shown. The\nexpected value E[f(x)] = 4.991 represents the mean\nmodel output across the training dataset. Feature\ncontributions collectively reduce the prediction to\nf(x) = 4.746. Among these, the pause-related fea-\ntures - NFP, MLUP, and NUP - exhibit the most pro-\nnounced negative impact, decreasing the prediction\nby 0.22, 0.16, and 0.1, respectively. Conversely,\nMLR has the strongest positive effect, increasing\nthe prediction by 0.2. These findings suggest that\nthe interpreter may need to enhance pause manage-\nment by minimizing both the frequency and dura-\ntion of pauses while striving for more extended,\nuninterrupted speech production.\nThe SHAP waterfall plot for the TLQual pre-\ndiction of Sample 87 is depicted in Figure 7. The\nmodel\u2019s expected value is E[f(x)] = 5.258, with\nFigure 7: SHAP waterfall plot for the predicted TLQual\nscore of Sample 87.\nfeature contributions collectively increasing the\nprediction to 6.466. Among these, CN_RATIO\nis the most influential positive factor, increasing\nthe prediction by 0.47. Other contributing fea-\ntures include PC_RTTR, AP_RTTR, PV_RTTR,\nand AP_RATIO. Conversely, PP_RTTR exerts the\nmost significant negative influence, reducing the\nprediction by 0.44, with additional negative contri-\nbutions from PV_RATIO and MLC. These results\nindicate that the diversified and sophisticated use\nof CN, PC, AP, PV , and AP structures aligns with\ntypical language patterns in this context. However,\nexcessive use of PP structures (e.g. \u5728...\u4e0a,\u5f53...\u65f6)\nappears detrimental. Additionally, the negative im-\npact of MLC suggests that complex clauses could\nbe restructured into simpler sentences or reformu-\nlated using a topic-comment structure, a common\ngrammatical pattern in Chinese.\n5 Discussions\n5.1 Modeling effectiveness and the impact of\ndata augmentation\nOur analysis indicates that the selected machine\nlearning algorithms demonstrate robust perfor-\nmance on the augmented dataset, with RF yielding\nthe best results for InfoCom score estimation and\nXGBoost performing best on FluDel and TLQual.\n10\n\n--- Page 11 ---\nComparing with previous models that only per-\nform well on middle range scores but failing at\nlower and higher ranges (Han et al., 2025; Wang\nand Yuan, 2023), our results underscore the im-\nportance of data augmentation in improving model\nperformance, particularly for predicting scores at\nextreme ends of the scale.\n5.2 Global explanations of feature importance\nInformation completeness Our SHAP analysis\nidentifies the two neural-based metrics, BLEURT\nand CometKiwi, as having the greatest influence\non the global prediction of InfoCom scores, align-\ning with previous research by Han and Lu (2025).\nThe superior performance of BLEURT is likely at-\ntributable to its extensive pre-training on synthetic\ndata and its ability to incorporate diverse lexical\nand semantic signals, which enables the metric to\ncapture more nuanced linguistic patterns compared\nto BERTScore (Han and Lu, 2025). Conversely, the\nrelatively low performance of XCOMET may stem\nfrom a misalignment between its training paradigm\n(error annotation) and the assessment context (ana-\nlytical rubric scoring).\nFluency of delivery Our findings reveal that NFP\nhas the most pronounced negative impact on the\nmodel\u2019s global prediction of FluDel scores, fol-\nlowed by other pause-related features including\nMLUP, NUP, and MLFP, aligning with previous\nfindings (Yu and van Heuven, 2017). In contrast,\nmost speed fluency features (e.g. PSC, PTR, SR)\nexhibit small positive effects, although higher MLR\nvalues are linked to decreased predictions. We hy-\npothesize that the negative role of MLR stems from\nthe phenomenon that excessively long runs do not\nreflect controlled, fluent delivery but rather a form\nof \u201crun-on speech\u201d. The interpreter, under high\ncognitive load, may be rushing to output informa-\ntion without strategic pausing for emphasis or lis-\ntener comprehension (Lennon, 1990; Mead, 2005),\nleading to human raters perceiving the speech as\npoorly managed and difficult to process.\nTarget language quality Among the GPT-4o-\nannotated features, NWSE exerts a significant neg-\native effect on model predictions, underscoring the\nfoundational role of grammatical accuracy in hu-\nman judgments of language quality and mirroring\nfindings in L2 speaking assessment (Li et al., 2024).\nRegarding length-related features, MLS has a\npositive effect on predictions, aligning with Zech-\nner et al. (2017). MLC, on the other hand, yields anegative impact, which is in sharp contrast to find-\nings from other contexts such as L2 German and\nEnglish speaking (Neary-Sundquist, 2017; Bult\u00e9\nand Roothooft, 2020). This divergence likely stems\nfrom typological differences: the topic-comment\nstructure of Chinese prioritizes discourse coher-\nence, whereas the syntactic elaboration common in\nEnglish relies more heavily on complex clausal\ndependencies (Li and Thompson, 1989). This\nsuggests that in the Chinese interpreting context,\nlonger but less syntactically dense sentences are\nperceived as higher quality.\nAnother key finding is the superior predictive\nimportance of fine-grained features over coarse-\ngrained ones. Within this category, features reflect-\ning phraseological diversity (PC_RTTR, PP_RTTR,\nSP_RTTR, AP_RTTR, PV_RTTR) are more influ-\nential than the single phraseological complexity\nfeature (CN_RATIO). Furthermore, our results re-\nveal that Chinese-specific phraseological features\n(CN, PC, PP, PV) demonstrate greater importance\nthan their language-independent counterparts (SP,\nAP). Taken together, these findings point towards\nthe possibility that for E-C consecutive interpret-\ning, a robust assessment of language use relies less\non traditional measures of clausal complexity and\nmore on the diverse and accurate use of language-\nspecific phrasal units.\n5.3 The critical role of local explanations in\nautomated interpreting assessment\nLocal explanations in automated interpreting as-\nsessment offer significant value for both teach-\ning and learning practices (Kumar and Boulanger,\n2020; Tang et al., 2024; Gilpin et al., 2018; Rudin,\n2018; Linardatos et al., 2020). For educators, these\nexplanations provide actionable insights into the\nspecific strengths and weaknesses of individual stu-\ndents\u2019 performances by highlighting the features\nthat positively or negatively influence predicted\nscores. This enables teachers to tailor feedback and\ninstructional strategies to target precise areas for\nimprovement. For students, local explanations em-\npower students to take ownership of their learning\nby focusing on specific performance aspects that\nrequire attention.\nTake the SHAP-based local explanation of\nFluDel prediction for Sample 50 as an example.\nNotably, pause-related features emerge as the pri-\nmary detractors: NFP reduces the prediction by\n0.22, MLUP by 0.16, and NUP by 0.1, indicating\nthe student\u2019s difficulty with hesitation management.\n11\n\n--- Page 12 ---\nTo address this, instructors can implement targeted\nexercises such as shadowing practices, where stu-\ndents reproduce source language with minimal de-\nlay (Christoffels and de Groot, 2004). The instruc-\ntor could also implement targeted drills requiring\nstudents to deliver short segments without hesita-\ntion, progressively extending segment length while\nmonitoring pause reduction. For reducing unfilled\npauses specifically, anticipation exercises help stu-\ndents predict upcoming content elements, thereby\ndecreasing processing latency (Chmiel, 2020). Ad-\nditionally, instructing in chunking strategies - or-\nganizing information into manageable units - can\nalleviate cognitive load that frequently manifests\nas extended pauses (Thalmann et al., 2019).\nIn addition, the quantitative nature of SHAP\nvalues also allows instructors to prioritize inter-\nventions effectively. For this particular student,\naddressing filled pauses should take precedence\nover lengthy unfilled pauses, given its greater neg-\native impact (0.22 vs. 0.16). Furthermore, track-\ning these SHAP contributions longitudinally across\nmultiple performances enables instructors to moni-\ntor learning progression and intervention effective-\nness, facilitating timely adjustments to teaching\napproaches as needed.\n6 Conclusion\nIn this work, we propose an effective framework\nintegrating feature engineering, ML models, data\naugmentation, and XAI for the multi-dimensional\nassessment of interpreting quality. A key finding\nis that V AE-based data augmentation substantially\nenhances model performance. Global XAI analysis\nreveales that fidelity prediction is most sensitive to\nneural-embedding metrics such as BLEURT, while\nfluency scores are primarily influenced by break-\ndown features, with NFP exerting the strongest\nnegative effect. Target language quality, in turn,\ndepends heavily on language-specific phraseolog-\nical features, notably CN_RATIO. These global\ninsights are complemented by in-depth local ex-\nplanations, which effectively diagnose individual\nstrengths and weaknesses in performance. Looking\nforward, our method provides a promising direction\nin translating XAI-driven insights into pedagogical\ntools that deliver actionable feedback to trainees,\nthereby bridging the gap between automated as-\nsessment and student learning.References\nAlejandro Barredo Arrieta, Natalia D\u00edaz Rodr\u00edguez,\nJavier Del Ser, Adrien Bennetot, Siham Tabik, A. Bar-\nbado, Salvador Garc\u00eda, Sergio Gil-Lopez, Daniel\nMolina, Richard Benjamins, Raja Chatila, and Fran-\ncisco Herrera. 2019. Explainable artificial intelli-\ngence (xai): Concepts, taxonomies, opportunities\nand challenges toward responsible ai. Inf. Fusion ,\n58:82\u2013115.\nV . Balachandar and K. Venkatesh. 2024. A multi-\ndimensional student performance prediction model\n(mspp): An advanced framework for accurate aca-\ndemic classification and analysis. MethodsX , 14.\nHenri C. Barik. 1973. Simultaneous interpretation:\nTemporal and quantitative data. Language and\nSpeech , 16:237 \u2013 270.\nBram Bult\u00e9 and Hanne Roothooft. 2020. Investigating\nthe interrelationship between rated l2 proficiency and\nlinguistic complexity in l2 speech. System .\nSheila Castilho, Stephen Doherty, Federico Gaspari,\nand Joss Moorkens. 2018. Approaches to human and\nmachine translation quality assessment.\nNitesh V . Chawla, Kevin W. Bowyer, Lawrence O. Hall,\nand W. Philip Kegelmeyer. 2002. SMOTE: synthetic\nminority over-sampling technique. J. Artif. Intell.\nRes., 16:321\u2013357.\nLei Chen, Klaus Zechner, Su-Youn Yoon, Keelan\nEvanini, Xinhao Wang, Anastassia Loukina, Jidong\nTao, Lawrence Davis, Chong Min Lee, Min Ma,\nRobert Mundkowsky, Chi-Jui Lu, Chee Wee Leong,\nand Binod Gyawali. 2018. Automated scoring of non-\nnative speech using the speechrater sm v. 5.0 engine.\nETS Research Report Series , 2018:1\u201331.\nSijia Chen. 2024. Effects of subtitles on vocabulary\nlearning through videos: An exploration across differ-\nent learner types. The Journal of Specialised Trans-\nlation .\nAgnieszka Chmiel. 2020. Effects of simultaneous in-\nterpreting experience and training on anticipation, as\nmeasured by word-translation latencies.\nIngrid Christoffels and Annette M.B. de Groot. 2004.\nComponents of simultaneous interpreting: Compar-\ning interpreting with shadowing and paraphrasing.\nBilingualism: Language and Cognition , 7:227 \u2013 240.\nYanping Dong and Zhilong Xie. 2014. Contributions of\nsecond language proficiency and interpreting experi-\nence to cognitive control differences among young\nadult bilinguals. Journal of Cognitive Psychology ,\n26:506 \u2013 519.\nRuiji Fu, Zhengqi Pei, Jiefu Gong, Wei Song, Dechuan\nTeng, Wanxiang Che, Shijin Wang, Guoping Hu, and\nTing Liu. 2018. Chinese grammatical error diagnosis\nusing statistical and prior knowledge driven features\nwith probabilistic ensemble enhancement. In Pro-\nceedings of the 5th Workshop on Natural Language\n12\n\n--- Page 13 ---\nProcessing Techniques for Educational Applications,\nNLP-TEA@ACL 2018, Melbourne, Australia, July 19,\n2018 , pages 52\u201359. Association for Computational\nLinguistics.\nDaniel Gile. 2021. The effort models of interpreting as\na didactic construct. Advances in Cognitive Transla-\ntion Studies .\nLeilani H. Gilpin, David Bau, Ben Z. Yuan, Ayesha Ba-\njwa, Michael A. Specter, and Lalana Kagal. 2018. Ex-\nplaining explanations: An overview of interpretabil-\nity of machine learning. In 5th IEEE International\nConference on Data Science and Advanced Analytics,\nDSAA 2018, Turin, Italy, October 1-3, 2018 , pages\n80\u201389. IEEE.\nArthur C. Graesser, Danielle S. McNamara, Max M.\nLouwerse, and Zhiqiang Cai. 2004. Coh-metrix:\nAnalysis of text on cohesion and language. Behav-\nior Research Methods, Instruments, & Computers ,\n36:193\u2013202.\nNuno Miguel Guerreiro, Ricardo Rei, Daan van Stigt,\nLu\u00edsa Coheur, Pierre Colombo, and Andr\u00e9 F. T. Mar-\ntins. 2024. xcomet : Transparent machine transla-\ntion evaluation through fine-grained error detection.\nTrans. Assoc. Comput. Linguistics , 12:979\u2013995.\nChao Han. 2015. (para)linguistic correlates of perceived\nfluency in english-to-chinese simultaneous interpre-\ntation. International Journal of Comparative Litera-\nture and Translation Studies , 3:32\u201337.\nChao Han. 2018. Using analytic rating scales to as-\nsess english/chinese bi-directional interpretation: A\nlongitudinal rasch analysis of scale utility and rater\nbehavior. Linguistica Antverpiensia, New Series \u2013\nThemes in Translation Studies .\nChao Han and Xiaolei Lu. 2021. Can automated ma-\nchine translation evaluation metrics be used to as-\nsess students\u2019 interpretation in the language learning\nclassroom? Computer Assisted Language Learning ,\n36:1064 \u2013 1087.\nChao Han and Xiaolei Lu. 2025. Beyond bleu: Re-\npurposing neural-based metrics to assess interlingual\ninterpreting in tertiary-level language learning set-\ntings. Research Methods in Applied Linguistics .\nChao Han, Xiaolei Lu, and Shirong Chen. 2025. Mod-\neling rater judgments of interpreting quality: Ordinal\nlogistic regression using neural-based evaluation met-\nrics, acoustic fluency measures, and computational\nlinguistic indices. Research Methods in Applied Lin-\nguistics .\nChao Han and Liuyan Yang. 2023. Relating utterance\nfluency to perceived fluency of interpreting. Trans-\nlation and Interpreting Studies. The Journal of the\nAmerican Translation and Interpreting Studies Asso-\nciation , 18(3):421\u2013447.Chao Han, Binghan Zheng, Mingqing Xie, and Shirong\nChen. 2024. Raters\u2019 scoring process in assessment of\ninterpreting: an empirical study based on eye tracking\nand retrospective verbalisation. The Interpreter and\nTranslator Trainer , 18:400 \u2013 422.\nTianyi Han, Dechao Li, Xingcheng Ma, and Nan Hu.\n2022. Comparing product quality between transla-\ntion and paraphrasing: Using nlp-assisted evaluation\nframeworks. Frontiers in Psychology , 13.\nRenfen Hu, Jifeng Wu, and Xiaofei Lu. 2022a. Chinese\ncollocation analyzer (cca).\nRenfen Hu, Jifeng Wu, and Xiaofei Lu. 2022b. Word-\ncombination-based measures of phraseological diver-\nsity, sophistication, and complexity and their rela-\ntionship to second language chinese proficiency and\nwriting quality. Language Learning .\nYichen Jia and Vahid Aryadoust. 2023. The utility of\ngenerative artificial intelligence in rating interpreters\u2019\naccuracy: A case study of chatgpt-4.\nDiederik P. Kingma and Max Welling. 2014. Auto-\nencoding variational bayes. In 2nd International\nConference on Learning Representations, ICLR 2014,\nBanff, AB, Canada, April 14-16, 2014, Conference\nTrack Proceedings .\nVivekanandan Kumar and David Boulanger. 2020. Ex-\nplainable automated essay scoring: Deep learning\nreally has pedagogical value. In Frontiers in Educa-\ntion.\nKristopher Kyle. 2016. Measuring syntactic develop-\nment in l2 writing: Fine grained indices of syntactic\ncomplexity and usage-based indices of syntactic so-\nphistication.\nKristopher Kyle and Scott Andrew Crossley. 2017. As-\nsessing syntactic sophistication in l2 writing: A\nusage-based approach. Language Testing , 34:513\n\u2013 535.\nNgoc-Tien Le, Benjamin Lecouteux, and Laurent Be-\nsacier. 2016. Joint ASR and MT features for quality\nestimation in spoken language translation. In Pro-\nceedings of the 13th International Conference on\nSpoken Language Translation, IWSLT 2016, Seattle,\nWA, USA, December 8-9, 2016 . International Work-\nshop on Spoken Language Translation.\nSang-Bin Lee. 2019. Holistic assessment of consecutive\ninterpretation. Interpreting. International Journal of\nResearch and Practice in Interpreting .\nT. Lee. 2013. Incorporating translation into the lan-\nguage classroom and its potential impacts upon l2\nlearners.\nP. Alan Lennon. 1990. Investigating fluency in efl: A\nquantitative approach. Language Learning , 40:387\u2013\n417.\n13\n\n--- Page 14 ---\nC.N. Li and S.A. Thompson. 1989. Mandarin Chi-\nnese: A Functional Reference Grammar . Linguistics:\nAsian studies. University of California Press.\nWenchao Li, Zhentao Zhong, and Haitao Liu. 2024. A\ncomputer-assisted tool for automatically measuring\nnon-native japanese oral proficiency. Computer As-\nsisted Language Learning .\nJohn M. Linacre. 2002. What do infit and outfit, mean-\nsquare and standardized mean? Rasch Measurement\nTransactions , 16:878.\nPantelis Linardatos, Vasilis Papastefanopoulos, and\nSotiris B. Kotsiantis. 2020. Explainable ai: A re-\nview of machine learning interpretability methods.\nEntropy , 23.\nXiaofei Lu. 2010. Automatic analysis of syntactic com-\nplexity in second language writing. International\nJournal of Corpus Linguistics , 15:474\u2013496.\nXiaolei Lu and Chao Han. 2022. Automatic assessment\nof spoken-language interpreting based on machine-\ntranslation evaluation metrics. Interpreting. Interna-\ntional Journal of Research and Practice in Interpret-\ning.\nScott M. Lundberg and Su-In Lee. 2017. A unified ap-\nproach to interpreting model predictions. In Neural\nInformation Processing Systems .\nPeter Mead. 2005. Methodological issues in the study\nof interpreters\u2019 fluency.\nC. Mellinger. 2018. Translation, interpreting, and lan-\nguage studies: Confluence and divergence. Hispania ,\n100:241 \u2013 246.\nIbomoiye Domor Mienye and Yanxia Sun. 2022. A\nsurvey of ensemble learning: Concepts, algorithms,\napplications, and prospects. IEEE Access , 10:99129\u2013\n99149.\nAlhassan G. Mumuni and Fuseini Mumuni. 2022. Data\naugmentation: A comprehensive survey of modern\napproaches. Array , 16:100258.\nColleen A. Neary-Sundquist. 2017. Syntactic complex-\nity at multiple proficiency levels of l2 german speech.\nInternational Journal of Applied Linguistics , 27:242\u2013\n262.\nJohn M. Norris and Lourdes Ortega. 2009. Towards an\norganic approach to investigating caf in instructed\nsla: The case of complexity. Applied Linguistics ,\n30:555\u2013578.\nLourdes Ortega. 2003. Syntactic complexity measures\nand their relationship to l2 proficiency: A research\nsynthesis of college-level l2 writing. Applied Lin-\nguistics , 24:492\u2013518.\nLing Ouyang, Qianxi Lv, and Junying Liang. 2021. Coh-\nmetrix model-based automatic assessment of inter-\npreting quality.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalu-\nation of machine translation. In Proceedings of the\n40th Annual Meeting of the Association for Compu-\ntational Linguistics, July 6-12, 2002, Philadelphia,\nPA, USA , pages 311\u2013318. ACL.\nR. Parkavi, P. Karthikeyan, and A. Sheik Abdullah.\n2024. Enhancing personalized learning with ex-\nplainable ai: A chaotic particle swarm optimization\nbased decision support system. Appl. Soft Comput. ,\n156:111451.\nFranz P\u00f6chhacker. 2001. Quality assessment in confer-\nence and community interpreting. Meta: Translators\u2019\nJournal , 46:410\u2013425.\nMaja Popovic. 2015. chrf: character n-gram f-score\nfor automatic MT evaluation. In Proceedings of the\nTenth Workshop on Statistical Machine Translation,\nWMT@EMNLP 2015, 17-18 September 2015, Lis-\nbon, Portugal , pages 392\u2013395. The Association for\nComputer Linguistics.\nGaoqi Rao, Erhong Yang, and Baolin Zhang. 2020.\nOverview of nlptea-2020 shared task for chinese\ngrammatical error diagnosis. Proceedings of the\n6th Workshop on Natural Language Processing Tech-\nniques for Educational Applications .\nRicardo Rei, Marcos V . Treviso, Nuno Miguel Guer-\nreiro, Chrysoula Zerva, Ana C. Farinha, Christine\nMaroti, Jos\u00e9 G. C. de Souza, Taisiya Glushkova,\nDuarte M. Alves, Lu\u00edsa Coheur, Alon Lavie, and\nAndr\u00e9 F. T. Martins. 2022. Cometkiwi: Ist-unbabel\n2022 submission for the quality estimation shared\ntask. In Proceedings of the Seventh Conference on\nMachine Translation, WMT 2022, Abu Dhabi, United\nArab Emirates (Hybrid), December 7-8, 2022 , pages\n634\u2013645. Association for Computational Linguistics.\nMarco T\u00falio Ribeiro, Sameer Singh, and Carlos\nGuestrin. 2016. \"why should I trust you?\": Explain-\ning the predictions of any classifier. In Proceedings\nof the 22nd ACM SIGKDD International Conference\non Knowledge Discovery and Data Mining, San Fran-\ncisco, CA, USA, August 13-17, 2016 , pages 1135\u2013\n1144. ACM.\nCynthia Rudin. 2018. Stop explaining black box ma-\nchine learning models for high stakes decisions and\nuse interpretable models instead. Nature Machine\nIntelligence , 1:206 \u2013 215.\nThibault Sellam, Dipanjan Das, and Ankur P. Parikh.\n2020. BLEURT: learning robust metrics for text\ngeneration. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\nACL 2020, Online, July 5-10, 2020 , pages 7881\u20137892.\nAssociation for Computational Linguistics.\nShuxian Song. 2020. Fluency in simultaneous inter-\npreting of trainee interpreters : the perspectives of\ncognitive, utterance and perceived fluency.\n14\n\n--- Page 15 ---\nU. Stachl-Peier. 2020. Translating, interpreting, mediat-\ning: The cefr and advanced-level language learning\nin the digital age.\nCatherine Stenzl. 1983. Simultaneous interpretation:\nGroundwork towards a comprehensive model.\nCraig Stewart, Nikolai V ogler, Junjie Hu, Jordan L.\nBoyd-Graber, and Graham Neubig. 2018. Automatic\nestimation of simultaneous interpreter performance.\nInProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics, ACL 2018,\nMelbourne, Australia, July 15-20, 2018, Volume 2:\nShort Papers , pages 662\u2013666. Association for Com-\nputational Linguistics.\nXiaoyi Tang, Hongwei Chen, Daoyu Lin, and Kexin Li.\n2024. Incorporating fine-grained linguistic features\nand explainable ai into multi-dimensional automated\nwriting assessment. Applied Sciences .\nParveneh Tavakoli and Peter Skehan. 2005. Strategic\nplanning, task structure and performance testing.\nMirko Thalmann, Alessandra S. Souza, and Klaus Ober-\nauer. 2019. How does chunking help working mem-\nory? Journal of Experimental Psychology: Learning,\nMemory, and Cognition , 45:37\u201355.\nXiaoman Wang. 2024. Developing an automated graded\nassessment system for english/chinese interpreting.\nXiaoman Wang and Binhua Wang. 2022. Identifying flu-\nency parameters for a machine-learning-based auto-\nmated interpreting assessment system. Perspectives ,\n32:278 \u2013 294.\nXiaoman Wang and Lu Yuan. 2023. Machine-learning\nbased automatic assessment of communication in\ninterpreting. In Frontiers in Communication .\nZhiwei Wu. 2021. Chasing the unicorn? the feasibility\nof automatic assessment of interpreting fluency.\nWenting Yu and Vincent J. van Heuven. 2017. Pre-\ndicting judged fluency of consecutive interpreting\nfrom acoustic measures: Potential for automatic as-\nsessment and pedagogic implications. Interpreting ,\n19:47\u201368.\nKlaus Zechner, Su-Youn Yoon, S. Bhat, and Chee Wee\nLeong. 2017. Comparative evaluation of automated\nscoring of syntactic competence of non-native speak-\ners.Comput. Hum. Behav. , 76:672\u2013682.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.\nWeinberger, and Yoav Artzi. 2020. Bertscore: Evalu-\nating text generation with BERT. In 8th International\nConference on Learning Representations, ICLR 2020,\nAddis Ababa, Ethiopia, April 26-30, 2020 . OpenRe-\nview.net.\nYidi Zhang, Margarida Lucas, Pedro Bem-haja, and\nLu\u00eds Pedro. 2024a. The effect of student acceptance\non learning outcomes: Ai-generated short videos ver-\nsus paper materials. Comput. Educ. Artif. Intell. ,\n7:100286.Ziyin Zhang, Yikang Liu, Weifang Huang, Junyu Mao,\nRui Wang, and Hai Hu. 2024b. MELA: multilingual\nevaluation of linguistic acceptability. In Proceedings\nof the 62nd Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\nACL 2024, Bangkok, Thailand, August 11-16, 2024 ,\npages 2658\u20132674. Association for Computational\nLinguistics.\nNan Zhao. 2022. Speech disfluencies in consecutive\ninterpreting by student interpreters: The role of lan-\nguage proficiency, working memory, and anxiety.\nFrontiers in Psychology , 13.\n15\n\n--- Page 16 ---\nA More Details on Source Materials\nPassage Theme DESWC DESSL DESWLlt LDTTRa RDFRE RDFKGL RDL2\n1 Migration 185 19.32 5.11 0.72 35.25 15.05 13.37\n2 Migration 193 18.91 5.42 0.65 41.12 16.23 8.35\n3 Festival 182 19.75 5.16 0.75 29.74 16.51 8.90\n4 Festival 191 18.86 5.32 0.68 42.18 14.66 10.20\n5 Social equality 179 19.44 5.23 0.74 45.36 13.28 7.39\n6 Social equality 185 19.06 5.28 0.66 33.33 15.73 11.46\nTable 6: Basic information on the six passages used in the interpreting tasks. DESWC: word count; DESSL:\nsentence length (number of words); DESWLlt: word length (mean); LDTTRa: lexical densitiy (type-token ratio);\nRDFRE: Flesch Reading Ease; RDFKGL: Flesch-Kincaid Grade Level; RDL2: L2 Readability.\nB Rater Training Procedures\nTo familiarize the raters with the assessment procedures, we arranged an online training session via a\nvideo conferencing software. Two authors of this study introduced the source texts and corresponding\nreference interpretations, and clarified certain key terms within the analytic rating scales (e.g. \u201cfilled\npauses\u201d, \u201clong silence\u201d, and \u201cexcessive repairs\u201d). Raters were actively encouraged to seek clarification on\nany aspect of the rating task, so as to ensure a shared understanding of the assessment criteria. To enhance\nrating consistency, pre-scored, representative interpretations from each band were played and analyzed\ncollectively. This served to illustrate the typical features associated with different performance levels.\nSubsequently, the raters independently completed trial ratings of five additional interpretations. After that,\nthey engaged in a collaborative discussion, comparing their scores and providing justifications for their\nrating decisions. The formal rating was also conducted remotely, with each rater receiving secure online\naccess to all necessary materials, including the source texts, reference translations, and the anonymized\ninterpretations. To ensure ample time for thorough evaluation, raters were given two weeks to complete\ntheir assessments.\nDimension Infit MnSq Outfit MnSq Rater reliability Person Separation reliability\nRater 1 1.02 1.01\nInfoCom Rater 2 0.84 0.79 0.97 0.83\nRater 3 0.78 0.65\nRater 1 1.15 1.08\nFluDel Rater 2 1.04 1.01 0.98 0.81\nRater 3 0.89 0.75\nRater 1 1.07 0.99\nTLQual Rater 2 0.90 0.93 0.96 0.76\nRater 3 0.82 1.04\nTable 7: Infit, outfit, rater reliability, and person separation reliability statistics from the MFRM analysis.\n16\n\n--- Page 17 ---\nC Prompt for Chinese Grammatical Error Diagnosis\nPrompt for Chinese grammatical error diagnosis\n**Instruction**\nYou are a Chinese grammar expert. Your task is to diagnose and correct grammatical errors in\nChinese sentences or longer texts. Follow the steps and guidelines below meticulously:\n1. Error Detection and Analysis Order\nAnalyze the input text for potential errors in the following priority order:\n- Redundancy (R): Repeated words or characters that unnecessarily clutter the sentence.\n- Missing Words (M): Omitted words or particles that make the sentence incomplete or ambiguous.\n- Word Selection (S): Inappropriate or inaccurate word choices that should be replaced by more\ncontext-appropriate terms.\n- Word Order (W): Incorrect arrangement of words or phrases that distorts the intended meaning.\n2. Error Description and Correction\nFor each detected error:\n- Describe the nature of the error.\n- Propose a correction that clarifies the meaning while preserving the original intent.\n- Assign a confidence score (0\u20131) representing your certainty in the correction. (Scores closer to 1\nindicate high confidence.)\n3. Re-examination for Low Confidence\nIf an error receives a confidence score below 0.7, re-examine it by asking:\n- \u201cDoes this correction improve the sentence without introducing ambiguity?\u201d\n- \u201cIs the error type correctly classified?\u201d\nRevise the correction if necessary before finalizing your output.\n4. Handling Special Cases\nThe following special cases should be addressed:\n- Filled Pauses: Words such as \u201c \u5443\u201d, \u201c\u989d\u201d, and \u201c\u55ef\u201d (and similar utterance markers) are considered\nfillers and should be ignored during error analysis. Do not report these as grammatical errors.\n- Repeated Phrases, False Starts, and Self-Corrections: Only analyze the final output of the\nsentence. Ignore any extraneous parts resulting from repetition or self-correction.\n5. Output Formatting\nFor every detected error, output an entry using the following format:\n[sentence_id, start_index, end_index, error_type, corrected_text, confidence]\n- sentence_id: A unique identifier for the sentence (or text segment) under analysis.\n- start_index and end_index: The character positions (based on the sentence\u2019s index) where the\nerror occurs.\n- error_type: One of the following codes: R (Redundancy), M (Missing Words), S (Word Selection),\nor W (Word Order).\n- corrected_text: The proposed correction.\n- confidence: A numerical value between 0 and 1 that represents your certainty.\n6. Multiple Errors\nNote that a sentence or text passage may contain more than one error. In such cases, output each\nerror as a separate entry.\n7. Examples for Illustration\n17\n\n--- Page 18 ---\n- Example 1: Simple Redundancy Correction\n- Input:\u6211\u6628\u5929\u53bb\u5b66\u6821\u5b66\u6821\u4e86\u3002\n- Expected Output: [1, 6, 7, R, \u5b66\u6821, 0.95]\n- Reasoning: The particle \u201c \u4e86\u201d is repeated unnecessarily (positions 6\u20137). The extra \u201c \u4e86\u201d should\nbe removed. High confidence is given due to the unambiguous redundancy.\n- Example 2: Word Order Correction\n- Input:\u4ed6\u8dd1\u5f97\u5feb\u6bd4\u6211\u8fd8\u3002\n- Expected Output: [2, 4, 6, W, \u6bd4\u6211\u8fd8\u5feb, 0.85]\n- Reasoning: The phrase \u201c \u8dd1\u5f97\u5feb\u6bd4\u6211\u8fd8\u201d is mis-ordered. Reordering to \u201c \u6bd4\u6211\u8fd8\u5feb\u201d aligns\nwith natural Chinese word order.\n- Example 3: Word selection improvement\n- Input:\u4e0d\u53d7\u76d1\u7ba1\u7684\u79fb\u6c11\u6d3b\u52a8\u4f1a\u9020\u6210\u79fb\u6c11\u8fdb\u5165\u8bb8\u591a\u5371\u9669\u7684\u8def\u7ebf,\u4e5f\u4f1a\u906d\u5230\u4eba\u53e3\u8d29\u5356\u8005\u7684\u6b8b\n\u5fcd\u9b54\u722a\u3002\n- Expected Output:\nEntry 1: [3, 10, 11, S, \u201c \u8ba9\u201d, 0.95]\nEntry 2: [3, 25, 28, S, \u201c \u79fb\u6c11\u4f1a\u843d\u5165\u201d, 0.90]\n- Reasoning: Entry 1: \u201c \u9020\u6210\u201d is not a suitable verb. Entry 2: \u201c \u906d\u5230\u201d is not a natural collocation\nwith \u201c\u9b54\u722a\u201d. The verb \u201c \u843d\u5165\u201d better conveys that immigrants \u201cfall into\u201d the clutches ( \u9b54\u722a) of\nhuman traffickers. Additionally, the extra adverb \u201c \u4e5f\u201d is unnecessary.\n- Example 4: Handling Special Cases\n- Input:\u5443\uff0c\u6211\u89c9\u5f97\u4eca\u5929\u7684\u4f1a\u8bae\uff0c\u55ef\uff0c\u6ca1\u5565\u5927\u95ee\u9898\u3002\n- Expected Output: No error entries.\n- Reasoning: \u201c \u5443\u201d or \u201c\u55ef\u201d are neglected. Only the final phrasing after self-corrections and filler\npauses should be examined for genuine grammatical issues.\n18\n\n--- Page 19 ---\nD Complete Feature Statistics\nFeatureMean SD Skewness Kurtosis\nRaw Aug. Raw Aug. Raw Aug. Raw Aug.\nInfoCom features\nCometKiwi 0.51 0.51 0.10 0.06 0.13 0.22 -0.53 0.82\nBertScore 0.96 0.96 0.01 0.00 -0.73 -1.20 -0.32 1.56\nchrF 0.11 0.11 0.02 0.02 0.14 0.16 -0.55 1.23\nBLEURT-20 0.51 0.50 0.13 0.07 1.14 1.87 2.85 1.52\nXCOMET 0.18 0.17 0.11 0.06 1.06 1.66 1.77 0.96\nFluDel features\nNUP 34.05 34.57 14.95 15.26 0.78 0.73 1.24 2.16\nMLUP 1.00 0.94 0.61 0.46 2.01 2.53 5.35 7.62\nMLFP 0.35 0.35 0.14 0.08 -0.06 -0.12 0.80 5.01\nNFP 15.72 15.40 8.41 6.03 0.68 0.51 0.89 1.29\nMLR 16.99 17.00 2.60 1.58 0.53 0.58 1.04 2.35\nPSC 197.78 196.12 55.36 34.18 0.76 0.84 0.55 0.97\nPTR 0.63 0.59 0.12 0.09 0.18 0.19 -0.76 0.80\nMLS 0.26 0.26 0.04 0.02 0.96 1.17 3.64 1.72\nSR 1.73 1.72 0.48 0.39 0.81 0.87 1.58 1.91\nAR 3.87 3.86 0.53 0.32 0.03 0.07 2.08 1.61\nNRSA 3.75 3.76 2.99 1.82 1.25 1.57 1.72 0.58\nNPSA 0.78 0.80 1.28 1.16 2.28 2.27 5.75 2.33\nNRLFP 0.18 0.17 0.54 0.42 3.51 5.37 9.03 7.39\nNRLUP 1.05 0.99 1.33 1.21 1.81 1.98 4.02 3.26\nTLQual features\nNRW 1.68 1.70 0.51 0.55 0.44 0.12 1.23 2.57\nNMW 2.17 2.15 0.62 0.67 -0.43 -0.30 2.26 1.95\nNWSE 4.13 4.16 1.15 1.48 1.33 0.68 2.34 3.39\nNWOE 0.98 1.02 0.34 0.36 0.88 0.57 1.95 2.64\nMLC 16.87 16.84 2.70 2.46 0.79 0.79 1.47 5.38\nMLTU 19.57 20.04 3.46 3.87 0.98 1.05 1.34 2.32\nNCPS 3.69 3.68 1.28 1.64 1.49 2.35 3.58 2.64\nNTPS 3.20 3.27 1.11 1.55 1.35 2.17 2.70 1.44\nTOTAL_RTTR 5.41 5.45 0.94 0.81 0.18 -0.10 1.18 3.61\nVO_RATIO 0.21 0.22 0.08 0.04 0.18 -0.11 1.15 2.76\nVO_RTTR 2.55 2.58 0.62 0.54 -0.22 -0.70 2.01 2.23\nSP_RATIO 0.22 0.23 0.09 0.11 0.81 0.68 3.94 3.50\nSP_RTTR 2.54 2.52 0.60 0.52 -0.06 -0.40 -0.19 1.24\nAN_RATIO 0.08 0.09 0.04 0.02 0.24 -0.14 3.08 1.12\nAN_RTTR 1.48 1.51 0.65 0.47 -0.64 -1.27 2.29 1.16\nAP_RATIO 0.37 0.39 0.09 0.05 -0.02 -0.48 -0.69 2.86\nAP_RTTR 3.18 3.19 0.77 0.62 0.21 -0.06 3.40 1.09\nCN_RATIO 0.01 0.01 0.02 0.01 1.71 1.63 -0.30 0.82\nCN_RTTR 0.40 0.42 0.58 0.44 0.98 0.70 2.39 1.18\nPP_RATIO 0.03 0.03 0.03 0.02 1.61 2.18 -1.45 1.32\nPP_RTTR 0.67 0.71 0.56 0.39 -0.15 -0.69 4.48 2.71\nPV_RATIO 0.04 0.05 0.04 0.02 1.78 2.01 5.64 2.63\nPV_RTTR 0.89 0.89 0.57 0.41 -0.41 -1.05 -0.79 1.96\nPC_RATIO 0.04 0.04 0.04 0.03 1.22 1.32 1.41 3.62\nPC_RTTR 0.88 0.91 0.64 0.71 -0.26 -0.82 -1.01 2.78\nTable 8: Descriptive statistics of all extracted features on raw data and augmented data.\n19\n\n--- Page 20 ---\nE Case Studies of Model Prediction Errors\nSample 47 From the original dataset; RF model True score: 6.34; Predicted score: 5.29\nKey features BLEURT: 0.66; CometKiwi: 0.62; chrF: 0.07; BERTScore: 0.97; xCOMET: 0.35\nKey features\n(M\u00b1SD) for Score\n6 samplesBLEURT (0.54 \u00b10.13); CometKiwi (0.54 \u00b10.10); chrF (0.13 \u00b10.02); BERTScore\n(0.96\u00b10.01); xCOMET (0.21\u00b10.12)\nError analysisThe model underestimates the InfoCom score of Sample 47 by 1.05. Upon examining\nsamples within the 5.5\u20136.5 score range, we observe that Sample 47 exhibits a partic-\nularly low chrF score (0.07). This value is more than one standard deviation below\nthe mean (0.11) for this feature among samples in this range. Analysis of the corre-\nsponding student transcript reveals a tendency to reorder sentence components during\ninterpretation, though key information in the source speech is interpreted faithfully\ninto the target language. For instance, when interpreting an \u201cif...then...\u201d sentence, the\nstudent processes the \u201cthen\u201d clause before the \u201cif\u201d clause, which results in reduced\nn-gram matching and consequently a lower chrF score for this sample.\nTable 9: Cases of notable disagreement between machine and human scores for InfoCom.\nSample 95 From the original dataset; XGBoost model True score: 4.73; Predicted score: 3.48\nSample featuresNFP: 13; MLR: 20.64; MLUP: 1.18; NUP: 42; MLFP: 0.26; PSC: 185; SR: 1.53;\nPTR: 0.41; NRSA: 2; MLS: 0.25\nFeatures (M \u00b1SD)\nfor Score 5 samplesNFP (18.16 \u00b15.66); MLR (17.13 \u00b11.11); MLUP (1.02 \u00b10.11); NUP (30.4 \u00b16.73);\nMLFP (0.38 \u00b10.12); PSC (195.96 \u00b113.44); SR (1.72 \u00b10.25); PTR (0.45 \u00b10.24); NRSA\n(4.4\u00b13.55); MLS (0.27\u00b10.04)\nError analysisFor Sample 95, the model underestimates the FluDel score by 1.25 points. Analysis of\nthis sample\u2019s features reveals notably high values for MLUP (1.18) and NUP (42), both\napproximately two standard deviations above their respective means. Also, the speech\nrate (1.53) is lower than the mean (1.72). Collectively, these feature values likely lead\nthe model to interpret this sample as having more significant breakdowns and reduced\nspeaking speed. However, qualitative examination of the corresponding student\nrecording offers a contrasting perspective. While the student does exhibit longer\nand more frequent pauses than average, these disfluencies predominantly occur at\nboundaries between semantic units within sentences. For human rates, this placement\nof pauses does not hurt perceived fluency as much as within-phrase disfluencies, which\nmay explain why the actual perceived score is higher than the model\u2019s prediction\nbased on these automated features.\nTable 10: Cases of notable disagreement between machine and human scores for FluDel.\n20\n\n--- Page 21 ---\nSample 62 From the original dataset; XGBoost model True score: 6.22; Predicted score: 5.01\nKey featuresCN_RATIO: 0; PC_RTTR: 0; MLS: 19.57; PP_RTTR: 1; SP_RTTR: 0.71; AP_RTTR:\n2; MLC: 14; NWSE: 0.26; PV_RTTR: 0.89; MLTU: 17.11\nKey features\n(M\u00b1SD) for Score\n6 samplesCN_RATIO (0.01 \u00b10.01); PC_RTTR (0.99 \u00b10.39); MLS (21.36 \u00b17.18); PP_RTTR\n(0.81 \u00b10.29); SP_RTTR (2.54 \u00b10.35); AP_RTTR (3.23 \u00b10.44); MLC (17.08 \u00b11.30);\nNWSE (1.69\u00b10.74); PV_RTTR (0.98\u00b10.34); MLTU (19.73\u00b11.56)\nError analysisThe predicted score is 1.21 points lower than that assigned by human raters. A\ncontributing factor to this discrepancy may be the notable absence of two specific\nChinese structures, CN and PC expressions, in the student\u2019s interpretation. Instead,\nthe students frequently employ expressions characteristic of Westernized Chinese, a\nstyle influenced by Western language structures. While human raters appear to find\nthese alternative expressions acceptable within the context of the task, the model likely\npenalizes the lack of the expected native Chinese forms, leading to the observed lower\nscores.\nTable 11: Cases of notable disagreement between machine and human scores for TLQual.\n21",
  "project_dir": "artifacts/projects/enhanced_cs.CL_2508.10860v1_From_Black_Box_to_Transparency_Enhancing_Automate",
  "communication_dir": "artifacts/projects/enhanced_cs.CL_2508.10860v1_From_Black_Box_to_Transparency_Enhancing_Automate/.agent_comm",
  "assigned_at": "2025-08-16T20:49:46.851164",
  "status": "assigned"
}